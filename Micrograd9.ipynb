{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec201ec2",
   "metadata": {},
   "source": [
    "Una volta estratti i parametri modificabili all'interno del NN, possiamo ora procedere con il meccanismo di gradient descent al fine di aggiustare questi parametri al fine di minimizzare la loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9c14286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5c7cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    # Costruttore\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        \n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None   # Funzione che non fa nulla\n",
    "        self._prev = set(_children) \n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    # Metodo utilizzato da Python per la visualizzazione dell'oggetto. Consente di impostare un visualizzazione user friendly\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    # Definisce l'operatore per la somma (+). Quando trova l'operatore +, Python chiama questo metodo\n",
    "    def __add__(self, other):\n",
    "        \n",
    "        # Per gestire la somma di un Value con una costante\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        \n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        # Definisco una funzione che applica la chain rule per calcolare la derivata dei 2 termini rispetto all'output.\n",
    "        # Dal momento che la local derivative per una somma è = 1, moltiplico 1 * la derivata del risultato della somma\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    # Metodo x negazione (-Value()). Utilizzato dal metodo successivo x sottrazione\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    # Definisce l'operatore sottrazione\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    \n",
    "    # Definisce l'operatore per la prodottp (*). Quando trova l'operatore *, Python chiama questo metodo\n",
    "    def __mul__(self, other):\n",
    "        \n",
    "         # Per gestire la somma di un Value con una costante\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        \n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        # Definisco una funzione che applica la chain rule per calcolare la derivata dei 2 fattori del prodotto\n",
    "        # rispetto all'output.\n",
    "        # Dal momento che la local derivative per un prodotto è = al valore dell'altro fattore,\n",
    "        # moltiplico l'altro fattore * la derivata del prodotto\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # Questo metodo viene richiamato da Python in caso di 2 * Value(). Restituirebbe un errore e quindi Python\n",
    "    # richiama questo metodo per dare l'opportunità di gestire l'errore invertendo l'ordine degli operatori\n",
    "    def __rmul__(self,other):\n",
    "        return self * other\n",
    "    \n",
    "    def __radd__(self,other):\n",
    "        return self + other\n",
    "    \n",
    "    # Questo metodo viene richiamato dabPython per l'elevazione a potenza (**n). Viene utilizzato per implementare la divisione\n",
    "    # in quanto a / b corrisponde a a * (1/b) e quindi a * (b**-1)\n",
    "    #\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), 'only supporting int/float powers for now'\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "        \n",
    "        # Sappiamo che la derivata di una potenza è uguale a n x**(n-1)\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    # Metodo per la divisione: utilizza la moltiplicazione del valore del dividendo \n",
    "    # per il divisone elevato a potenza -1. Quindi utilizza il metodo precedente\n",
    "    #\n",
    "    def __truediv__(self,other):\n",
    "        return self * other**-1\n",
    "    \n",
    "    # Metodo per il calcolo della funzione tanh. \n",
    "    # E' possibile utilizzare la funzione \"composta\" invece che le singole operazioni che la compongono in quanto \n",
    "    # siamo in grado di calcolarne la derivata\n",
    "    #\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "        \n",
    "        # Definisco una funzione che applica la chain rule per calcolare la derivata della funzione tanh \n",
    "        # rispetto all'output.\n",
    "        # Dal momento che la local derivative per tanh è = (1 - x**2), applicando la chain rule\n",
    "        # moltiplico questa local derivative* la derivata del prodotto\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    # Calcolo dell'a funzione exp: costituisce una delle operazioni che fanno parte della formula di tanh\n",
    "    # \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward= _backward\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e36b091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \n",
    "    # Definisco il costruttore che riceve in input il numero di input al Neuron\n",
    "    def __init__(self, nin):\n",
    "        \n",
    "        # Inizializzo i weigths e bias sulla base del numero di inputs\n",
    "        #\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "        \n",
    "    # Metodo richiamato quando l'istanza della classe viene invocata passando il parametro previsto\n",
    "    # il questo caso il metodo esegue il forward pass\n",
    "    def __call__(self, x):\n",
    "        # w * x +b\n",
    "        # Nel nostro caso dovremo moltiplicare tutti gli elementi di w per tutti gli elementi di x (pairwise)\n",
    "        # zip riceve 2 iterators e crea un iterator che tratte le tuple date dai 2 iterator\n",
    "        act = sum(wi*xi for wi, xi in zip(self.w, x)) +self.b\n",
    "        \n",
    "        # Per ragioni di efficienza sarebbe preferibile\n",
    "        # act = sum((wi*xi for wi, xi in zip(self.w,x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "    \n",
    "    # Creo una lista composta da tutti i weigths e dal bias (trasformato in list x consentire la somma di List + List = List)\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "class Layer:\n",
    "    \n",
    "    # Il cotruttore riceve in input il numero di input e il numero di neurons che costituiscono il layer\n",
    "    #\n",
    "    def __init__(self, nin, nout):\n",
    "        \n",
    "        # Il layer è costituito da una lista di nout Neurons\n",
    "        #\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    # Calcoliamo la lista degli output di ogni singolo neuron che compone il layer\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        # return outs\n",
    "        # Per gestire il fatto che può ritornare un singolo numero\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "    # Creo una lista composta da tutti i weigths e dal bias di tutti i neuron che compongono il layer\n",
    "    def parameters(self):\n",
    "        # params = [neuron.parameters() for neuron in self.neurons]\n",
    "        params = [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "        return params\n",
    "    \n",
    "\n",
    "class MLP:\n",
    "    \n",
    "    # Riceve il numero di inputs e la lista con le dimensioni dei layers che costutuiscono l'MLP\n",
    "    def __init__(self, nin, nouts):\n",
    "        \n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "   \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)                                               \n",
    "        return x    \n",
    "\n",
    "    # Creo una lista composta da tutti i weigths e dal bias di tutti i neuron di tutti i layers che compongono il MLP\n",
    "    def parameters(self):\n",
    "        params = [p for layer in self.layers for p in layer.parameters()]\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8ff975",
   "metadata": {},
   "source": [
    "Definisco i datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "504b2019",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "ys = [1.0, -1.0, -1,0, 1.0] # targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa419214",
   "metadata": {},
   "source": [
    "Inizializzo l'MLP ed eseguo il forward pass per ottenere le prediction (basate sui valori di inizializzazione dei param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c275ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.5148315070801582),\n",
       " Value(data=0.06388505770520524),\n",
       " Value(data=0.45604447265991066),\n",
       " Value(data=0.5091537274871519)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = MLP(3, [4, 4, 1])\n",
    "\n",
    "ypred = [n(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352577e4",
   "metadata": {},
   "source": [
    "Dopo il foreward pass posso calcolare il valore della loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0b6a59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=3.746542907108057)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c2e84",
   "metadata": {},
   "source": [
    "Innesco il meccanismo di backprop al fine di calcolare i gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c027e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471fa2ae",
   "metadata": {},
   "source": [
    "A titolo di esempio visualizzo il valore di 1 parametro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3900af5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8143905006996706"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.layers[0].neurons[0].w[0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca247c",
   "metadata": {},
   "source": [
    "e visualizzo il corrispondente gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6b64e32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07828044382760115"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.layers[0].neurons[0].w[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "62966efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff37cb",
   "metadata": {},
   "source": [
    "A questo punto è possibile iterare i parametri al fine di aggiustarli per minimizzare la loss function.\n",
    "Per fare ciò è necessario aggiungere al valore del parametro per il valore del parametro stesso moltiplicato x un valore molto piccolo x il gradient (di segno contrario in quanto la loss function deve diminuire mantre il gradient indica la direzione dell'incremento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aea7d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in n.parameters():\n",
    "    p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bbc57696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8222185450824306"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.layers[0].neurons[0].w[0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e194ec77",
   "metadata": {},
   "source": [
    "Posso rifare un forward pass e ricalcolare il loss che dovrebbe essersi ridotto rispetto a prima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "58aa0e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=2.1308389616573935)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = [n(x) for x in xs]\n",
    "loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9e09ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7f079e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in n.parameters():\n",
    "    p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "15fc5c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=1.5751268934142049)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = [n(x) for x in xs]\n",
    "loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f793a831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=2.493067183162297)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "for p in n.parameters():\n",
    "    p.data += -0.1 * p.grad\n",
    "ypred = [n(x) for x in xs]\n",
    "loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "11af7e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.588082593034952)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "for p in n.parameters():\n",
    "    p.data += -0.1 * p.grad\n",
    "ypred = [n(x) for x in xs]\n",
    "loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c717b1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9968650419192825),\n",
       " Value(data=-0.9999999806140204),\n",
       " Value(data=-0.9999998300916838),\n",
       " Value(data=0.9957767757744413)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ddcbf9",
   "metadata": {},
   "source": [
    "Come si può vedere ora le prediction sono molto vicine ai targets (1.0, -1.0, -1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867e264d",
   "metadata": {},
   "source": [
    "Vediamo ora come implementare il training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e1aa0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "ys = [1.0, -1.0, -1,0, 1.0] # targets\n",
    "\n",
    "n = MLP(3, [4, 4, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4b02f02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.09861242677690318\n",
      "1 0.15555810902052636\n",
      "2 0.09748605759091127\n",
      "3 0.14666298581357107\n",
      "4 0.09218749146861113\n",
      "5 0.13397104442866134\n",
      "6 0.08853318271757107\n",
      "7 0.12489930172494398\n",
      "8 0.08503915074207803\n",
      "9 0.11712653857268703\n",
      "10 0.08198989990745767\n",
      "11 0.11076779661812479\n",
      "12 0.07934524095183296\n",
      "13 0.10555794300845334\n",
      "14 0.07707219132655405\n",
      "15 0.10129138481563378\n",
      "16 0.07512779835691485\n",
      "17 0.09778915977201577\n",
      "18 0.07346535609251173\n",
      "19 0.09489853939007181\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "    \n",
    "    # forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    \n",
    "    # calcolo loss\n",
    "    loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "    \n",
    "    # backward pass (azzerando il gradient di tutti i parametri)\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "    loss.backward()\n",
    "    \n",
    "    # update parameter\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.05 * p.grad\n",
    "    print(k, loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "693c4061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.8889009846882733),\n",
       " Value(data=-0.9926469404321338),\n",
       " Value(data=-0.918060115664377),\n",
       " Value(data=0.275294998241691)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90f56aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-env]",
   "language": "python",
   "name": "conda-env-pytorch-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
